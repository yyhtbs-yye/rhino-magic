# ddp_trainer_demo.py
from __future__ import annotations

import os, copy, socket, random
from contextlib import ExitStack, nullcontext
from datetime import datetime

import numpy as np
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.utils.data import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from pathlib import Path

from trainer.simple_trainer import GlobalStep
from trainer.loggers.tensorboard import TensorBoardLogger
from trainer.utils.build_components import build_module

from tqdm import tqdm

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

def is_dist_ready():
    return dist.is_available() and dist.is_initialized()

@torch.no_grad()
def broadcast_module_state(module: torch.nn.Module, src_rank: int = 0):
    """Broadcast *all* tensors in state_dict (params + buffers) from src_rank."""
    if not is_dist_ready():
        return
    for t in module.state_dict().values():
        if torch.is_tensor(t):
            dist.broadcast(t, src_rank)

@torch.no_grad()
def copy_buffers(dst: torch.nn.Module, src: torch.nn.Module):
    """Optional: keep non-learnable buffers (e.g., BN running stats) matched to src."""
    for (n1, b1), (n2, b2) in zip(dst.named_buffers(), src.named_buffers(), strict=False):
        if torch.is_tensor(b1) and torch.is_tensor(b2) and b1.shape == b2.shape:
            b1.copy_(b2)

def get_primary_trainable_module(boat):
    # Prefer a DDP-wrapped model if present, else any trainable module
    for m in boat.models.values():
        if isinstance(m, DDP):
            return m
    for m in boat.models.values():
        if isinstance(m, torch.nn.Module) and any(p.requires_grad for p in m.parameters()):
            return m
    return None

def find_free_port():
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("127.0.0.1", 0))
        return s.getsockname()[1]

def ddp_setup_env(rank, world_size, addr, port):
    os.environ.setdefault("MASTER_ADDR", addr)
    os.environ.setdefault("MASTER_PORT", str(port))
    os.environ["RANK"] = str(rank)
    os.environ["WORLD_SIZE"] = str(world_size)
    os.environ["LOCAL_RANK"] = str(rank)

def seed_everything(seed, rank):
    random.seed(seed + rank)
    np.random.seed(seed + rank)
    torch.manual_seed(seed + rank)
    torch.cuda.manual_seed_all(seed + rank)

def is_rank0() -> bool:
    return (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0

def ddp_no_sync_all(boat, enabled: bool):
    """Enter no_sync() on all DDP-wrapped modules if enabled."""
    if not enabled:
        return nullcontext()
    stack = ExitStack()
    for m in boat.models.values():
        if isinstance(m, DDP):
            stack.enter_context(m.no_sync())
    return stack

def wrap_models_with_ddp(boat, device, *, fup_by_key = None, broadcast_buffers=True):
    """Wrap each trainable nn.Module in boat.models[...] with DDP."""
    fup_by_key = fup_by_key or {}
    for k, module in list(boat.models.items()):
        if not isinstance(module, torch.nn.Module):
            continue
        module.to(device)
        # If nothing requires grad (e.g., EMA), don't wrap in DDP.
        if not any(p.requires_grad for p in module.parameters()):
            boat.models[k] = module
            continue

        ddp = DDP(
            module,
            device_ids=[device.index] if device.type == "cuda" else None,
            output_device=device.index if device.type == "cuda" else None,
            find_unused_parameters=bool(fup_by_key.get(k, False)),  # default False for perf
            broadcast_buffers=broadcast_buffers,
        )
        boat.models[k] = ddp

# ------------------------------
# DDP Trainer
# ------------------------------

class DDPTrainer:
    """
    One-process-per-GPU trainer. No torchrun needed.
    - Trainer owns DDP/AMP/no_sync.
    - Boat owns training_step (loss -> backward -> maybe step), DDP-agnostic.
    """
    def __init__(
        self,
        boat,
        trainer_config,
        callback_configs=None,
        experiment_name = None,
        run_folder=None,
        resume_from=None,
        devices = None,
    ):
        
        self.train_states = {}
        self.callback_configs = callback_configs

        self.trainer_config = trainer_config
        self.devices          = devices
        self.train_states['val_check_steps']    = trainer_config.get("val_check_steps", None)
        self.train_states['val_check_epochs']   = trainer_config.get("val_check_epochs", None)
        self.train_states['state_save_steps']   = trainer_config.get("state_save_steps", None)
        self.train_states['state_save_epochs']  = trainer_config.get("state_save_epochs", None)
        self.train_states['target_metric_name'] = boat.validation_config.get('target_metric_name', 'psnr')
        self.train_states['save_images']        = trainer_config.get("save_images", False)
        self.train_states['experiment_name']    = experiment_name

        # ───── model(s) and checkpoint restore ─────
        if resume_from:
            ckpt_path   = Path(resume_from)
            self.train_states['run_folder'] = Path(run_folder)
            boat, meta = boat.load_state(ckpt_path)
            self.train_states['global_step'] = GlobalStep(meta.get("global_step", 0))
            self.train_states['start_epoch'] = meta.get("epoch", 0)
        else:
            self.train_states['run_folder']  = Path(run_folder) if run_folder else None
            self.train_states['global_step'] = GlobalStep(0)
            self.train_states['start_epoch'] = 0

        # keep a CPU template; each worker deep-copies it
        self._boat_template = copy.deepcopy(boat)

    def fit(self, data_module):
        use_cuda = torch.cuda.is_available()
        total = torch.cuda.device_count() if use_cuda else 0
        if not use_cuda:
            world_size = 1
        else:
            world_size = total if self.devices is None else max(1, min(len(self.devices), total))

        if world_size == 1:
            addr, port = "127.0.0.1", find_free_port()
            train_worker(
                rank=0,
                world_size=1,
                addr=addr,
                port=port,
                devices=self.devices,
                boat_template=self._boat_template,
                trainer_config=self.trainer_config,
                data_module=data_module,
                train_states=self.train_states,
                callback_configs=self.callback_configs,
            )
            return

        os.environ.setdefault("MASTER_ADDR", "127.0.0.1")
        os.environ.setdefault("MASTER_PORT", str(find_free_port()))
        addr, port = os.environ["MASTER_ADDR"], int(os.environ["MASTER_PORT"])

        # spawn – pass only picklable state
        mp.start_processes(
            train_worker,
            nprocs=world_size,
            args=(world_size, addr, port, self.devices, self._boat_template, self.trainer_config, 
                  data_module, self.train_states, self.callback_configs),
            start_method="spawn",
            join=True,
        )

# ------------------------------
# Worker (top-level function for spawn)
# ------------------------------

def train_worker(
    rank,
    world_size,
    addr,
    port,
    devices,
    boat_template,
    trainer_config,
    data_module,
    train_states, 
    callback_configs,
):
    ddp_setup_env(rank, world_size, addr, port)

    use_cuda = torch.cuda.is_available()
    device = torch.device(f"cuda:{devices[rank]}" if use_cuda else "cpu")
    if use_cuda:
        torch.cuda.set_device(device)

    backend = "nccl" if use_cuda else "gloo"
    dist.init_process_group(backend=backend, rank=rank, world_size=world_size)
    seed_everything(trainer_config.get('seed', 42), rank)

    # Build per-rank boat & wrap with DDP
    boat = copy.deepcopy(boat_template)
    boat.attach_global_step(train_states['global_step'])
    boat.build_optimizers()
    boat.build_losses()
    boat.build_metrics()
    boat.to(device)

    wrap_models_with_ddp(boat, device, fup_by_key=trainer_config.get('fup_by_key'), broadcast_buffers=True)

    # Identify gradient-free modules as EMA-like (or mark them in your code with a flag)
    ema_keys = [
        k for k, m in boat.models.items()
        if isinstance(m, torch.nn.Module) and not any(p.requires_grad for p in m.parameters()) and 'ema' in k
    ]

    if is_rank0():
        callbacks   = [build_module(cb) for cb in (callback_configs or [])]
        logger     = TensorBoardLogger(log_dir=train_states['run_folder'],
                                       name=train_states['experiment_name'])
        train_states['valid_epoch_records'] = {}
        train_states['valid_step_records'] = {}
        

    primary = get_primary_trainable_module(boat)

    # Optionally initialize EMA from primary, then broadcast once so every rank matches
    if ema_keys and primary is not None and is_dist_ready():
        if is_rank0():
            src = primary.module if isinstance(primary, DDP) else primary
            for k in ema_keys:
                try:
                    # params only; optionally also copy buffers to start identical
                    boat.models[k].load_state_dict(src.state_dict(), strict=False)
                except Exception:
                    pass
        for k in ema_keys:
            broadcast_module_state(boat.models[k], src_rank=0)

    precision = trainer_config.get('precision', None)
    # AMP context + scaler
    if precision is None:
        amp_dtype = None
    elif precision == "bf16-mixed":
        amp_dtype = torch.bfloat16
    elif precision == "16-mixed":
        amp_dtype = torch.float16
    else:
        amp_dtype = None

    if amp_dtype is not None and device.type == "cuda":
        autocast_ctx = lambda: torch.autocast(device_type="cuda", dtype=amp_dtype)
    else:
        autocast_ctx = lambda: nullcontext()

    if precision is not None:
        scaler = torch.cuda.amp.GradScaler(enabled=(precision == "16-mixed"))
    else:
        scaler = None
    
    # Dataloaders
    train_loader = data_module.make_train_loader(world_size, rank)
    valid_loader = data_module.make_valid_loader() if is_rank0() else None

    # Train
    for epoch in range(trainer_config.get("max_epochs", 10)):
        if isinstance(getattr(train_loader, "sampler", None), DistributedSampler):
            train_loader.sampler.set_epoch(epoch)

        if is_rank0():
            for cb in callbacks:
                cb.on_epoch_start(train_states, boat, epoch)

        boat.train()

        print(f"rank_{rank} before train_loader")
        iterator = enumerate(train_loader)
        print(f"rank_{rank} after train_loader")
        
        total_batches = len(train_loader) if hasattr(train_loader, "__len__") else None
        if is_rank0() and tqdm is not None:
            iterator = tqdm(iterator, total=total_batches, unit="batch",
                            desc=f"Epoch {epoch} | loss N/A | {datetime.now():%Y-%m-%d %H:%M:%S}")

        for step, (batch_idx_batch) in enumerate(iterator, start=1):
            # dataloader yields dict already; support both tuple or single
            if isinstance(batch_idx_batch, tuple):
                batch_idx, batch = batch_idx_batch
            else:
                batch_idx, batch = step - 1, batch_idx_batch

            # bump global step
            if boat.get_global_step() > 0 and epoch == 0 and batch_idx == 0:
                boat.global_step -= 1
            boat.global_step += 1

            accumulate = trainer_config.get("accumulate", 1)
            # accumulation bookkeeping
            microstep = (step - 1) % accumulate
            is_boundary = (accumulate <= 1) or (microstep == accumulate - 1)
            skip_allreduce = (accumulate > 1) and (not is_boundary)

            # Move batch
            batch = move_to_device(batch, device)

            if is_rank0():
                for cb in callbacks:
                    cb.on_batch_start(train_states, boat, batch, batch_idx)

            # no_sync across all DDP modules until boundary; AMP is trainer-owned
            with ddp_no_sync_all(boat, enabled=skip_allreduce):
                with autocast_ctx():
                    losses = boat.training_step(
                        batch, batch_idx, epoch, 
                        scaler=scaler,
                        accumulate=accumulate,
                        microstep=microstep,
                    )

            ema_broadcast_steps = trainer_config.get("ema_broadcast_steps", 1)  # 1 = every boundary

            if ema_keys and primary is not None and is_boundary:
                # Broadcast at your chosen cadence
                if (boat.get_global_step() % ema_broadcast_steps) == 0:
                    for k in ema_keys:
                        broadcast_module_state(boat.models[k], src_rank=0)
            # --- end EMA ---

            # Rank-0: display mean loss across ranks
            total_loss = losses.get("total_loss", None)
            loss_scalar = torch.tensor(0.0, device=device)

            if total_loss is not None:
                loss_scalar = total_loss.detach()
                
                if dist.is_initialized():
                    dist.all_reduce(loss_scalar, op=dist.ReduceOp.SUM)
                    loss_scalar /= world_size
                
                if is_rank0():
                    boat.log_train_losses(logger, losses)

                    if tqdm is not None:
                        iterator.set_description(
                            f"Epoch {epoch} | loss {loss_scalar.item():.4f} | {datetime.now():%Y-%m-%d %H:%M:%S}"
                        )
            if is_rank0():
                for cb in callbacks:
                    cb.on_batch_end(train_states, boat, batch, batch_idx, total_loss)

        # validation
        if is_rank0():

            if train_states['val_check_epochs'] is not None and epoch % train_states['val_check_epochs'] == 0:
                avg_loss = run_validation(boat, valid_loader, train_states, callbacks, 
                                            device, logger)
                train_states['valid_epoch_records'][epoch] = {'avg_loss': avg_loss.detach().cpu()}
        
            if train_states['state_save_epochs'] is not None and epoch % train_states['state_save_epochs'] == 0:
                state_path = boat.save_state(train_states['run_folder'], 'boat_state', global_step=train_states['global_step'](), epoch=epoch+1)
                
                if epoch not in train_states['valid_epoch_records']:
                    train_states['valid_epoch_records'][epoch] = {}
                train_states['valid_epoch_records'][epoch]['state_path'] = state_path

            for cb in callbacks:
                cb.on_epoch_end(train_states, boat, epoch)
    
    if is_rank0():
        for cb in callbacks:
            cb.on_train_end(train_states, boat)

    dist.destroy_process_group()

def run_validation(boat, val_dataloader, train_states, callbacks, 
                   device, logger):

    for cb in callbacks:
        cb.on_validation_start(train_states, boat)

    boat.eval()
    aggr_metrics = {}
    with torch.no_grad():
        for batch_idx, batch in enumerate(val_dataloader):
            batch = move_to_device(batch, device)
            metrics, named_imgs = boat.validation_step(batch, batch_idx)
            for cb in callbacks:
                cb.on_validation_batch_end(train_states, boat, batch, batch_idx, outputs=metrics)
            # average each metric in metrics
            for key, value in metrics.items():
                if key not in aggr_metrics:
                    aggr_metrics[key] = metrics[key]
                else:
                    aggr_metrics[key] += metrics[key]
    
            boat.visualize_validation(logger, named_imgs, batch_idx)

        for key in aggr_metrics:
            aggr_metrics[key] /= batch_idx

    if not aggr_metrics:
        raise ValueError("Validation loop produced no losses.")

    boat.log_valid_metrics(logger, aggr_metrics)
    
    for cb in callbacks:
        cb.on_validation_end(train_states, boat)

    if train_states['target_metric_name'] not in aggr_metrics:
        raise KeyError(f"'{train_states['target_metric_name']}' not found in validation metrics: {list(aggr_metrics)}")

    return aggr_metrics[train_states['target_metric_name']]

def move_to_device(batch, device):
    if isinstance(batch, dict):
        return {k: move_to_device(v, device) for k, v in batch.items()}
    if isinstance(batch, (list, tuple)):
        return type(batch)(move_to_device(x, device) for x in batch)
    if hasattr(batch, "to"):
        return batch.to(device, non_blocking=True)
    return batch

